\hypertarget{internals-of-the-nim-compiler}{%
\section{Internals of the Nim
Compiler}\label{internals-of-the-nim-compiler}}

\begin{description}
\item[Author]
Andreas Rumpf
\item[Version]
1.3.5
\end{description}

"Abstraction is layering ignorance on top of reality." -\/- Richard
Gabriel

\hypertarget{directory-structure}{%
\subsection{Directory structure}\label{directory-structure}}

The Nim project's directory structure is:

\begin{longtable}[]{@{}ll@{}}
\toprule
Path & Purpose\tabularnewline
\midrule
\endhead
\texttt{bin} & generated binary files\tabularnewline
\texttt{build} & generated C code for the installation\tabularnewline
\texttt{compiler} & the Nim compiler itself; note that this code has
been translated from a bootstrapping version written in Pascal, so the
code is \textbf{not} a poster child of good Nim code\tabularnewline
\texttt{config} & configuration files for Nim\tabularnewline
\texttt{dist} & additional packages for the distribution\tabularnewline
\texttt{doc} & the documentation; it is a bunch of reStructuredText
files\tabularnewline
\texttt{lib} & the Nim library\tabularnewline
\texttt{web} & website of Nim; generated by \texttt{nimweb} from the
\texttt{*.txt} and \texttt{*.nimf} files\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{bootstrapping-the-compiler}{%
\subsection{Bootstrapping the
compiler}\label{bootstrapping-the-compiler}}

Compiling the compiler is a simple matter of running:

\begin{verbatim}
nim c koch.nim
./koch boot
\end{verbatim}

For a release version use:

\begin{verbatim}
nim c koch.nim
./koch boot -d:release
\end{verbatim}

And for a debug version compatible with GDB:

\begin{verbatim}
nim c koch.nim
./koch boot --debuginfo --linedir:on
\end{verbatim}

The \texttt{koch} program is Nim's maintenance script. It is a
replacement for make and shell scripting with the advantage that it is
much more portable. More information about its options can be found in
the \href{koch.html}{koch} documentation.

\hypertarget{coding-guidelines}{%
\subsection{Coding Guidelines}\label{coding-guidelines}}

\begin{itemize}
\tightlist
\item
  Use CamelCase, not underscored\_identifiers.
\item
  Indent with two spaces.
\item
  Max line length is 80 characters.
\item
  Provide spaces around binary operators if that enhances readability.
\item
  Use a space after a colon, but not before it.
\item
  {[}deprecated{]} Start types with a capital \texttt{T}, unless they
  are pointers/references which start with \texttt{P}.
\end{itemize}

See also the \href{apis.html}{API naming design} document.

\hypertarget{porting-to-new-platforms}{%
\subsection{Porting to new platforms}\label{porting-to-new-platforms}}

Porting Nim to a new architecture is pretty easy, since C is the most
portable programming language (within certain limits) and Nim generates
C code, porting the code generator is not necessary.

POSIX-compliant systems on conventional hardware are usually pretty easy
to port: Add the platform to \texttt{platform} (if it is not already
listed there), check that the OS, System modules work and recompile Nim.

The only case where things aren't as easy is when the garbage collector
needs some assembler tweaking to work. The standard version of the GC
uses C's \texttt{setjmp} function to store all registers on the hardware
stack. It may be necessary that the new platform needs to replace this
generic code by some assembler code.

\hypertarget{runtime-type-information}{%
\subsection{Runtime type information}\label{runtime-type-information}}

\emph{Runtime type information} (RTTI) is needed for several aspects of
the Nim programming language:

\begin{description}
\item[Garbage collection]
The most important reason for RTTI. Generating traversal procedures
produces bigger code and is likely to be slower on modern hardware as
dynamic procedure binding is hard to predict.
\item[Complex assignments]
Sequences and strings are implemented as pointers to resizeable buffers,
but Nim requires copying for assignments. Apart from RTTI the compiler
could generate copy procedures for any type that needs one. However,
this would make the code bigger and the RTTI is likely already there for
the GC.
\end{description}

We already know the type information as a graph in the compiler. Thus we
need to serialize this graph as RTTI for C code generation. Look at the
file \texttt{lib/system/hti.nim} for more information.

\hypertarget{rebuilding-the-compiler}{%
\subsection{Rebuilding the compiler}\label{rebuilding-the-compiler}}

After an initial build via {sh build\_all.sh} on posix or
{build\_all.bat} on windows, you can rebuild the compiler as follows: *
{nim c koch} if you need to rebuild koch * {./koch boot -d:release} this
ensures the compiler can rebuild itself (use {koch} instead of {./koch}
on windows), which builds the compiler 3 times.

A faster approach if you don't need to run the full bootstrapping
implied by {koch boot}, is the following: * {pathto/nim c -\/-lib:lib
-d:release -o:bin/nim\_temp compiler/nim.nim} Where {pathto/nim} is any
nim binary sufficiently recent (eg {bin/nim\_cources} built during
bootstrap or {\$HOME/.nimble/bin/nim} installed by {choosenim 1.2.0})

You can pass any additional options such as {-d:leanCompiler} if you
don't need certain features or {-d:debug -\/-stacktrace:on
-\/-excessiveStackTrace -\/-stackTraceMsgs} for debugging the compiler.
See also {[}Debugging the
compiler{]}(intern.html\#debugging-the-compiler).

\hypertarget{debugging-the-compiler}{%
\subsection{Debugging the compiler}\label{debugging-the-compiler}}

You can of course use GDB or Visual Studio to debug the compiler (via
\texttt{-\/-debuginfo\ -\/-lineDir:on}). However, there are also lots of
procs that aid in debugging:

\begin{verbatim}
\end{verbatim}

To create a new compiler for each run, use \texttt{koch\ temp}:

\begin{verbatim}
./koch temp c /tmp/test.nim
\end{verbatim}

\texttt{koch\ temp} creates a debug build of the compiler, which is
useful to create stacktraces for compiler debugging. See also
{[}Rebuilding the compiler{]}(intern.html\#rebuilding-the-compiler) if
you need more control.

\hypertarget{bisecting-for-regressions}{%
\subsection{Bisecting for regressions}\label{bisecting-for-regressions}}

\texttt{koch\ temp} returns 125 as the exit code in case the compiler
compilation fails. This exit code tells \texttt{git\ bisect} to skip the
current commit.:

\begin{verbatim}
git bisect start bad-commit good-commit
git bisect run ./koch temp -r c test-source.nim
\end{verbatim}

You can also bisect using custom options to build the compiler, for
example if you don't need a debug version of the compiler (which runs
slower), you can replace {./koch temp} by explicit compilation command,
see {[}Rebuilding the compiler{]}(intern.html\#rebuilding-the-compiler).

\hypertarget{the-compilers-architecture}{%
\subsection{The compiler's
architecture}\label{the-compilers-architecture}}

Nim uses the classic compiler architecture: A lexer/scanner feds tokens
to a parser. The parser builds a syntax tree that is used by the code
generator. This syntax tree is the interface between the parser and the
code generator. It is essential to understand most of the compiler's
code.

In order to compile Nim correctly, type-checking has to be separated
from parsing. Otherwise generics cannot work.

\hypertarget{short-description-of-nims-modules}{%
\subsubsection{Short description of Nim's
modules}\label{short-description-of-nims-modules}}

\begin{longtable}[]{@{}ll@{}}
\toprule
Module & Description\tabularnewline
\midrule
\endhead
nim & main module: parses the command line and calls
\texttt{main.MainCommand}\tabularnewline
main & implements the top-level command dispatching\tabularnewline
nimconf & implements the config file reader\tabularnewline
syntaxes & dispatcher for the different parsers and
filters\tabularnewline
filter\_tmpl & standard template filter
(\texttt{\#?\ stdtempl})\tabularnewline
lexbase & buffer handling of the lexical analyser\tabularnewline
lexer & lexical analyser\tabularnewline
parser & Nim's parser\tabularnewline
renderer & Nim code renderer (AST back to its textual
form)\tabularnewline
options & contains global and local compiler options\tabularnewline
ast & type definitions of the abstract syntax tree (AST) and node
constructors\tabularnewline
astalgo & algorithms for containers of AST nodes; converting the AST to
YAML; the symbol table\tabularnewline
passes & implement the passes manager for passes over the
AST\tabularnewline
trees & some algorithms for nodes; this module is less
important\tabularnewline
types & module for traversing type graphs; also contain several helpers
for dealing with types\tabularnewline
sigmatch & contains the matching algorithm that is used for proc
calls\tabularnewline
semexprs & contains the semantic checking phase for
expressions\tabularnewline
semstmts & contains the semantic checking phase for
statements\tabularnewline
semtypes & contains the semantic checking phase for types\tabularnewline
seminst & instantiation of generic procs and types\tabularnewline
semfold & contains code to deal with constant folding\tabularnewline
semthreads & deep program analysis for threads\tabularnewline
evals & contains an AST interpreter for compile time
evaluation\tabularnewline
pragmas & semantic checking of pragmas\tabularnewline
idents & implements a general mapping from identifiers to an internal
representation (\texttt{PIdent}) that is used so that a simple
id-comparison suffices to establish whether two Nim identifiers are
equivalent\tabularnewline
ropes & implements long strings represented as trees for lazy
evaluation; used mainly by the code generators\tabularnewline
transf & transformations on the AST that need to be done before code
generation\tabularnewline
cgen & main file of the C code generator\tabularnewline
ccgutils & contains helpers for the C code generator\tabularnewline
ccgtypes & the generator for C types\tabularnewline
ccgstmts & the generator for statements\tabularnewline
ccgexprs & the generator for expressions\tabularnewline
extccomp & this module calls the C compiler and linker; interesting if
you want to add support for a new C compiler\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{the-syntax-tree}{%
\subsubsection{The syntax tree}\label{the-syntax-tree}}

The syntax tree consists of nodes which may have an arbitrary number of
children. Types and symbols are represented by other nodes, because they
may contain cycles. The AST changes its shape after semantic checking.
This is needed to make life easier for the code generators. See the
"ast" module for the type definitions. The \href{macros.html}{macros}
module contains many examples how the AST represents each syntactic
structure.

\hypertarget{how-the-rtl-is-compiled}{%
\subsection{How the RTL is compiled}\label{how-the-rtl-is-compiled}}

The \texttt{system} module contains the part of the RTL which needs
support by compiler magic (and the stuff that needs to be in it because
the spec says so). The C code generator generates the C code for it,
just like any other module. However, calls to some procedures like
\texttt{addInt} are inserted by the CCG. Therefore the module
\texttt{magicsys} contains a table (\texttt{compilerprocs}) with all
symbols that are marked as \texttt{compilerproc}. \texttt{compilerprocs}
are needed by the code generator. A \texttt{magic} proc is not the same
as a \texttt{compilerproc}: A \texttt{magic} is a proc that needs
compiler magic for its semantic checking, a \texttt{compilerproc} is a
proc that is used by the code generator.

\hypertarget{compilation-cache}{%
\subsection{Compilation cache}\label{compilation-cache}}

The implementation of the compilation cache is tricky: There are lots of
issues to be solved for the front- and backend.

\hypertarget{general-approach-ast-replay}{%
\subsubsection{General approach: AST
replay}\label{general-approach-ast-replay}}

We store a module's AST of a successful semantic check in a SQLite
database. There are plenty of features that require a sub sequence to be
re-applied, for example:

\begin{verbatim}
\end{verbatim}

The solution is to \textbf{re-play} the module's top level statements.
This solves the problem without having to special case the logic that
fills the internal seqs which are affected by the pragmas.

In fact, this describes how the AST should be stored in the database, as
a "shallow" tree. Let's assume we compile module \texttt{m} with the
following contents:

\begin{verbatim}
var x*: int = 90
{.compile: "foo.c".}
proc p = echo "p"
proc q = echo "q"
static:
  echo "static"
\end{verbatim}

Conceptually this is the AST we store for the module:

\begin{verbatim}
var x*
{.compile: "foo.c".}
proc p
proc q
static:
  echo "static"
\end{verbatim}

The symbol's \texttt{ast} field is loaded lazily, on demand. This is
where most savings come from, only the shallow outer AST is
reconstructed immediately.

It is also important that the replay involves the \texttt{import}
statement so that dependencies are resolved properly.

\hypertarget{shared-global-compiletime-state}{%
\subsubsection{Shared global compiletime
state}\label{shared-global-compiletime-state}}

Nim allows \texttt{.global,\ compiletime} variables that can be filled
by macro invocations across different modules. This feature breaks
modularity in a severe way. Plenty of different solutions have been
proposed:

\begin{itemize}
\tightlist
\item
  Restrict the types of global compiletime variables to
  \texttt{Set{[}T{]}} or similar unordered, only-growable collections so
  that we can track the module's write effects to these variables and
  reapply the changes in a different order.
\item
  In every module compilation, reset the variable to its default value.
\item
  Provide a restrictive API that can load/save the compiletime state to
  a file.
\end{itemize}

(These solutions are not mutually exclusive.)

Since we adopt the "replay the top level statements" idea, the natural
solution to this problem is to emit pseudo top level statements that
reflect the mutations done to the global variable. However, this is MUCH
harder than it sounds, for example \texttt{squeaknim} uses this snippet:

\begin{verbatim}
\end{verbatim}

We can "replay" \texttt{stCode.add} only if the values of \texttt{st}
and \texttt{apicall} are known. And even then a hash table's
\texttt{add} with its hashing mechanism is too hard to replay.

In practice, things are worse still, consider
\texttt{someGlobal{[}i{]}{[}j{]}.add\ arg}. We only know the root is
\texttt{someGlobal} but the concrete path to the data is unknown as is
the value that is added. We could compute a "diff" between the global
states and use that to compute a symbol patchset, but this is quite some
work, expensive to do at runtime (it would need to run after every
module has been compiled) and would also break for hash tables.

We need an API that hides the complex aliasing problems by not relying
on Nim's global variables. The obvious solution is to use string keys
instead of global variables:

\begin{verbatim}
proc cachePut*(key: string; value: string)
proc cacheGet*(key: string): string
\end{verbatim}

However, the values being strings/json is quite problematic: Many lookup
tables that are built at compiletime embed \emph{proc vars} and types
which have no obvious string representation... Seems like AST diffing is
still the best idea as it will not require to use an alien API and works
with some existing Nimble packages, at least.

On the other hand, in Nim's future I would like to replace the VM by
native code. A diff algorithm wouldn't work for that. Instead the native
code would work with an API like \texttt{put}, \texttt{get}:

\begin{verbatim}
proc cachePut*(key: string; value: NimNode)
proc cacheGet*(key: string): NimNode
\end{verbatim}

The API should embrace the AST diffing notion: See the module
\texttt{macrocache} for the final details.

\hypertarget{methods-and-type-converters}{%
\subsubsection{Methods and type
converters}\label{methods-and-type-converters}}

In the following sections \emph{global} means \emph{shared between
modules} or \emph{property of the whole program}.

Nim contains language features that are \emph{global}. The best example
for that are multi methods: Introducing a new method with the same name
and some compatible object parameter means that the method's dispatcher
needs to take the new method into account. So the dispatching logic is
only completely known after the whole program has been translated!

Other features that are \emph{implicitly} triggered cause problems for
modularity too. Type converters fall into this category:

\begin{verbatim}
\end{verbatim}

\begin{verbatim}
if 1:
  echo "ugly, but should work"
\end{verbatim}

If in the above example module \texttt{B} is re-compiled, but \texttt{A}
is not then \texttt{B} needs to be aware of \texttt{toBool} even though
\texttt{toBool} is not referenced in \texttt{B} \emph{explicitly}.

Both the multi method and the type converter problems are solved by the
AST replay implementation.

\hypertarget{generics}{%
\paragraph{Generics}\label{generics}}

We cache generic instantiations and need to ensure this caching works
well with the incremental compilation feature. Since the cache is
attached to the \texttt{PSym} datastructure, it should work without any
special logic.

\hypertarget{backend-issues}{%
\subsubsection{Backend issues}\label{backend-issues}}

\begin{itemize}
\tightlist
\item
  Init procs must not be "forgotten" to be called.
\item
  Files must not be "forgotten" to be linked.
\item
  Method dispatchers are global.
\item
  DLL loading via \texttt{dlsym} is global.
\item
  Emulated thread vars are global.
\end{itemize}

However the biggest problem is that dead code elimination breaks
modularity! To see why, consider this scenario: The module \texttt{G}
(for example the huge Gtk2 module...) is compiled with dead code
elimination turned on. So none of \texttt{G}'s procs is generated at
all.

Then module \texttt{B} is compiled that requires \texttt{G.P1}. Ok, no
problem, \texttt{G.P1} is loaded from the symbol file and \texttt{G.c}
now contains \texttt{G.P1}.

Then module \texttt{A} (that depends on \texttt{B} and \texttt{G}) is
compiled and \texttt{B} and \texttt{G} are left unchanged. \texttt{A}
requires \texttt{G.P2}.

So now \texttt{G.c} MUST contain both \texttt{P1} and \texttt{P2}, but
we haven't even loaded \texttt{P1} from the symbol file, nor do we want
to because we then quickly would restore large parts of the whole
program.

\hypertarget{solution}{%
\paragraph{Solution}\label{solution}}

The backend must have some logic so that if the currently processed
module is from the compilation cache, the \texttt{ast} field is not
accessed. Instead the generated C(++) for the symbol's body needs to be
cached too and inserted back into the produced C file. This approach
seems to deal with all the outlined problems above.

\hypertarget{debugging-nims-memory-management}{%
\subsection{Debugging Nim's memory
management}\label{debugging-nims-memory-management}}

The following paragraphs are mostly a reminder for myself. Things to
keep in mind:

\begin{itemize}
\tightlist
\item
  If an assertion in Nim's memory manager or GC fails, the stack trace
  keeps allocating memory! Thus a stack overflow may happen, hiding the
  real issue.
\item
  What seem to be C code generation problems is often a bug resulting
  from not producing prototypes, so that some types default to
  \texttt{cint}. Testing without the \texttt{-w} option helps!
\end{itemize}

\hypertarget{the-garbage-collector}{%
\subsection{The Garbage Collector}\label{the-garbage-collector}}

\hypertarget{introduction}{%
\subsubsection{Introduction}\label{introduction}}

I use the term \emph{cell} here to refer to everything that is traced
(sequences, refs, strings). This section describes how the GC works.

The basic algorithm is \emph{Deferrent Reference Counting} with cycle
detection. References on the stack are not counted for better
performance and easier C code generation.

Each cell has a header consisting of a RC and a pointer to its type
descriptor. However the program does not know about these, so they are
placed at negative offsets. In the GC code the type \texttt{PCell}
denotes a pointer decremented by the right offset, so that the header
can be accessed easily. It is extremely important that \texttt{pointer}
is not confused with a \texttt{PCell} as this would lead to a memory
corruption.

\hypertarget{the-cellset-data-structure}{%
\subsubsection{The CellSet data
structure}\label{the-cellset-data-structure}}

The GC depends on an extremely efficient datastructure for storing a set
of pointers - this is called a \texttt{TCellSet} in the source code.
Inserting, deleting and searching are done in constant time. However,
modifying a \texttt{TCellSet} during traversal leads to undefined
behaviour.

\begin{verbatim}
proc cellSetInit(s: var TCellSet) # initialize a new set
proc cellSetDeinit(s: var TCellSet) # empty the set and free its memory
proc incl(s: var TCellSet, elem: PCell) # include an element
proc excl(s: var TCellSet, elem: PCell) # exclude an element

proc `in`(elem: PCell, s: TCellSet): bool # tests membership

iterator elements(s: TCellSet): (elem: PCell)
\end{verbatim}

All the operations have to perform efficiently. Because a Cellset can
become huge a hash table alone is not suitable for this.

We use a mixture of bitset and hash table for this. The hash table maps
\emph{pages} to a page descriptor. The page descriptor contains a bit
for any possible cell address within this page. So including a cell is
done as follows:

\begin{itemize}
\tightlist
\item
  Find the page descriptor for the page the cell belongs to.
\item
  Set the appropriate bit in the page descriptor indicating that the
  cell points to the start of a memory block.
\end{itemize}

Removing a cell is analogous - the bit has to be set to zero. Single
page descriptors are never deleted from the hash table. This is not
needed as the data structures needs to be rebuilt periodically anyway.

Complete traversal is done in this way:

\begin{verbatim}
for each page descriptor d:
  for each bit in d:
    if bit == 1:
      traverse the pointer belonging to this bit
\end{verbatim}

\hypertarget{further-complications}{%
\subsubsection{Further complications}\label{further-complications}}

In Nim the compiler cannot always know if a reference is stored on the
stack or not. This is caused by var parameters. Consider this example:

\begin{verbatim}
proc usage =
  var
    r: ref TNode
  setRef(r) # here we should not update the reference counts, because
            # r is on the stack
  setRef(r.left) # here we should update the refcounts!
\end{verbatim}

We have to decide at runtime whether the reference is on the stack or
not. The generated code looks roughly like this:

\begin{verbatim}
\end{verbatim}

Note that for systems with a continuous stack (which most systems have)
the check whether the ref is on the stack is very cheap (only two
comparisons).

\hypertarget{code-generation-for-closures}{%
\subsection{Code generation for
closures}\label{code-generation-for-closures}}

Code generation for closures is implemented by \texttt{lambda\ lifting}.

\hypertarget{design}{%
\subsubsection{Design}\label{design}}

A \texttt{closure} proc var can call ordinary procs of the default Nim
calling convention. But not the other way round! A closure is
implemented as a \texttt{tuple{[}prc,\ env{]}}. \texttt{env} can be nil
implying a call without a closure. This means that a call through a
closure generates an \texttt{if} but the interoperability is worth the
cost of the \texttt{if}. Thunk generation would be possible too, but
it's slightly more effort to implement.

Tests with GCC on Amd64 showed that it's really beneficial if the
'environment' pointer is passed as the last argument, not as the first
argument.

Proper thunk generation is harder because the proc that is to wrap could
stem from a complex expression:

\begin{verbatim}
\end{verbatim}

A thunk would need to call 'returnsDefaultCC{[}i{]}' somehow and that
would require an \emph{additional} closure generation... Ok, not really,
but it requires to pass the function to call. So we'd end up with 2
indirect calls instead of one. Another much more severe problem which
this solution is that it's not GC-safe to pass a proc pointer around via
a generic \texttt{ref} type.

Example code:

\begin{verbatim}
var add2 = add(2)
echo add2(5) #OUT 7
\end{verbatim}

This should produce roughly this code:

\begin{verbatim}
proc anon(y: int, c: PEnv): int =
  return y + c.x

proc add(x: int): tuple[prc, data] =
  var env: PEnv
  new env
  env.x = x
  result = (anon, env)

var add2 = add(2)
let tmp = if add2.data == nil: add2.prc(5) else: add2.prc(5, add2.data)
echo tmp
\end{verbatim}

Beware of nesting:

\begin{verbatim}
var add24 = add(2)(4)
echo add24(5) #OUT 11
\end{verbatim}

This should produce roughly this code:

\begin{verbatim}
PEnvY = ref object
  y: int
  ex: PEnvX

proc lambdaZ(z: int, ey: PEnvY): int =
return ey.ex.x + ey.y + z

proc lambdaY(y: int, ex: PEnvX): tuple[prc, data: PEnvY] =
var ey: PEnvY
new ey
ey.y = y
ey.ex = ex
result = (lambdaZ, ey)

proc add(x: int): tuple[prc, data: PEnvX] =
var ex: PEnvX
ex.x = x
result = (labmdaY, ex)

var tmp = add(2)
var tmp2 = tmp.fn(4, tmp.data)
var add24 = tmp2.fn(4, tmp2.data)
echo add24(5)
\end{verbatim}

We could get rid of nesting environments by always inlining inner anon
procs. More useful is escape analysis and stack allocation of the
environment, however.

\hypertarget{alternative}{%
\subsubsection{Alternative}\label{alternative}}

Process the closure of all inner procs in one pass and accumulate the
environments. This is however not always possible.

\hypertarget{accumulator}{%
\subsubsection{Accumulator}\label{accumulator}}

\begin{verbatim}
proc p =
  var delta = 7
  proc accumulator(start: int): proc(): int =
    var x = start-1
    result = proc (): int =
      x = x + delta
      inc delta
      return x

  var a = accumulator(3)
  var b = accumulator(4)
  echo a() + b()
\end{verbatim}

\hypertarget{internals}{%
\subsubsection{Internals}\label{internals}}

Lambda lifting is implemented as part of the \texttt{transf} pass. The
\texttt{transf} pass generates code to setup the environment and to pass
it around. However, this pass does not change the types! So we have some
kind of mismatch here; on the one hand the proc expression becomes an
explicit tuple, on the other hand the tyProc(ccClosure) type is not
changed. For C code generation it's also important the hidden formal
param is \texttt{void*} and not something more specialized. However the
more specialized env type needs to passed to the backend somehow. We
deal with this by modifying \texttt{s.ast{[}paramPos{]}} to contain the
formal hidden parameter, but not \texttt{s.typ}!

\hypertarget{integer-literals}{%
\subsubsection{Integer literals:}\label{integer-literals}}

In Nim, there is a redundant way to specify the type of an integer
literal. First of all, it should be unsurprising that every node has a
node kind. The node of an integer literal can be any of the following
values:

\begin{quote}
nkIntLit, nkInt8Lit, nkInt16Lit, nkInt32Lit, nkInt64Lit, nkUIntLit,
nkUInt8Lit, nkUInt16Lit, nkUInt32Lit, nkUInt64Lit
\end{quote}

On top of that, there is also the {typ} field for the type. It the kind
of the {typ} field can be one of the following ones, and it should be
matching the literal kind:

\begin{quote}
tyInt, tyInt8, tyInt16, tyInt32, tyInt64, tyUInt, tyUInt8, tyUInt16,
tyUInt32, tyUInt64
\end{quote}

Then there is also the integer literal type. This is a specific type
that is implicitly convertible into the requested type if the requested
type can hold the value. For this to work, the type needs to know the
concrete value of the literal. For example an expression {321} will be
of type {int literal(321)}. This type is implicitly convertible to all
integer types and ranges that contain the value {321}. That would be all
builtin integer types except {uint8} and {int8} where {321} would be out
of range. When this literal type is assigned to a new {var} or {let}
variable, it's type will be resolved to just {int}, not {int
literal(321)} unlike constants. A constant keeps the full {int
literal(321)} type. Here is an example where that difference matters.

\begin{verbatim}
proc foo(arg: int8) =
  echo "def"

const tmp1 = 123
foo(tmp1)  # OK

let tmp2 = 123
foo(tmp2) # Error
\end{verbatim}

In a context with multiple overloads, the integer literal kind will
always prefer the {int} type over all other types. If none of the
overloads is of type {int}, then there will be an error because of
ambiguity.

\begin{verbatim}
proc foo(arg: int) =
  echo "abc"
proc foo(arg: int8) =
  echo "def"
foo(123) # output: abc

proc bar(arg: int16) =
  echo "abc"
proc bar(arg: int8) =
  echo "def"

bar(123) # Error ambiguous call
\end{verbatim}

In the compiler these integer literal types are represented with the
node kind {nkIntLit}, type kind {tyInt} and the member {n} of the type
pointing back to the integer literal node in the ast containing the
integer value. These are the properties that hold true for integer
literal types.

\begin{quote}
n.kind == nkIntLit n.typ.kind == tyInt n.typ.n == n
\end{quote}

Other literal types, such as {uint literal(123)} that would
automatically convert to other integer types, but prefers to become a
{uint} are not part of the Nim language.

In an unchecked AST, the {typ} field is nil. The type checker will set
the {typ} field accordingly to the node kind. Nodes of kind {nkIntLit}
will get the integer literal type (e.g. {int literal(123)}). Nodes of
kind {nkUIntLit} will get type {uint} (kind {tyUint}), etc.

This also means that it is not possible to write a literal in an
unchecked AST that will after sem checking just be of type {int} and not
implicitly convertible to other integer types. This only works for all
integer types that are not {int}.
